{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4110fc18-35a4-46fc-8dcb-74a3ed2e9c66",
   "metadata": {},
   "source": [
    "# Apresentação:\n",
    "\n",
    "O objetivo desse código é explocar as capacidades da API da Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64e4ed3-39c3-4ef5-a9c1-1aba35ba9b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando classe pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Bibliotecas de suporte:\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c58d57-ed19-4223-873c-b191268e8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto utilizado:\n",
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure\n",
    "from your online store in Germany. Unfortunately, when I opened the package,\n",
    "I discovered to my horror that I had been sent an action figure of Megatron\n",
    "instead! As a lifelong enemy of the Decepticons, I hope you can understand my\n",
    "dilemma. To resolve the issue, I demand an exchange of Megatron for the\n",
    "Optimus Prime figure I ordered. Enclosed are copies of my records concerning\n",
    "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c256713-265f-46a9-a049-85d03b86475a",
   "metadata": {},
   "source": [
    "# Classificação de Texto:\n",
    "\n",
    "Apesar do tópico ser nomeado no livro como **classificação de texto**, o exemplo em questão trata de uma **análise de sentimentos**, um caso particular de classificação de texto, porém relevante o suficiente para ser pontuado por si só. Então vale esse _disclaimer_, apesar do nome, o que estamos fazendo aqui não é classificação de texto, é análise de sentimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2478dcc-8413-47c5-9889-d556ece45d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\amori\\anaconda3\\envs\\bndes\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instanciando Classificador de Texto:\n",
    "classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45a50efe-ecd1-4633-ae7b-31a068a94b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.901546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.901546"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classificando Texto:\n",
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dbebec-bc07-4e79-a926-82ba79703c3a",
   "metadata": {},
   "source": [
    "Observe que, para tarefas de análise de sentimentos, o pipeline só retorna um dos rótulos POSITIVO ou NEGATIVO, já que o outro pode ser inferido ao calcular **1-score**. O nada mais é do que uma forma de complementar a pontuação de um modelo em tarefas de **classificação binária**, como a **análise de sentimentos**. Ele indica que a probabilidade atribuída ao rótulo alternativo (não previsto pelo modelo) pode ser calculada subtraindo a pontuação dada ao rótulo previsto de 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e075f87-6bb2-4ba6-a835-4c7eeee27b2d",
   "metadata": {},
   "source": [
    "# Reconhecimento de Entidade Nomeada:\n",
    "\n",
    "No **NLP**, objetos do mundo real, como produtos, lugares e pessoas, são chamados de **entidades nomeadas**, e extraí-las do texto é chamado de **reconhecimento de entidades nomeadas (_Named Entity Recognition_ - NER)**. Podemos aplicar o **NER** carregando o `pipeline` correspondente e fornecendo a ele nossa avaliação de cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d645124a-517e-4d9c-a6aa-0d59b67306ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\amori\\anaconda3\\envs\\bndes\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instanciando classe:\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c778186-b97f-4723-801e-d696e95780df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.879010</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990859</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>36</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>Germany</td>\n",
       "      <td>90</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.556571</td>\n",
       "      <td>Mega</td>\n",
       "      <td>208</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.590255</td>\n",
       "      <td>##tron</td>\n",
       "      <td>212</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.669692</td>\n",
       "      <td>Decept</td>\n",
       "      <td>253</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.498349</td>\n",
       "      <td>##icons</td>\n",
       "      <td>259</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.775362</td>\n",
       "      <td>Megatron</td>\n",
       "      <td>350</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>367</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.812096</td>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>502</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score           word  start  end\n",
       "0          ORG  0.879010         Amazon      5   11\n",
       "1         MISC  0.990859  Optimus Prime     36   49\n",
       "2          LOC  0.999755        Germany     90   97\n",
       "3         MISC  0.556571           Mega    208  212\n",
       "4          PER  0.590255         ##tron    212  216\n",
       "5          ORG  0.669692         Decept    253  259\n",
       "6         MISC  0.498349        ##icons    259  264\n",
       "7         MISC  0.775362       Megatron    350  358\n",
       "8         MISC  0.987854  Optimus Prime    367  380\n",
       "9          PER  0.812096      Bumblebee    502  511"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reconhecendo Entidades Nomeadas: \n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be68742-6458-446b-a70d-9b2da0da76c7",
   "metadata": {},
   "source": [
    "Pode-se ver que o `pipeline` detectou todas as entidades e também atribuiu uma categoria, como **ORG** (organização), **LOC** (localização) ou **PER** (pessoa) a cada uma delas. Aqui, usamos o atributo `aggregation_strategy` para agrupar as palavras de acordo com as previsões do modelo. Por exemplo, a entidade \"Optimus Prime\" é composta de duas palavras, mas é atribuída a uma única categoria: **MISC** (diversos). \n",
    "\n",
    "As pontuações nos dizem o quão confiante o modelo estava sobre as entidades que identificou. Podemos ver que ele estava menos confiante sobre \"Decepticons\" e a primeira ocorrência de \"Megatron\", ambos os quais ele falhou em agrupar como uma única entidade.\n",
    "\n",
    "Note aqueles símbolos _hash_ estranhos (#) na coluna de palavras da tabela anterior? Estes são produzidos pelo modelo\r\n",
    "tokenizer, que divide palavras em unidades atômicas chamadas tokens.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803555a-1b50-4eab-9c0e-046ce3527c7a",
   "metadata": {},
   "source": [
    "# Resposta a perguntas (_Question Answering_):\n",
    "\n",
    "Na tarefa de **resposta às perguntas** (_Question Answering_), fornecemos ao modelo uma passagem de texto chamada **contexto**, junto\n",
    "com uma pergunta cuja resposta gostaríamos de extrair. O modelo então retorna a extensão do texto correspondente à resposta. Esse approach talvez lembre um pouco o **RAG (_Retrieval-Augmented Generation_)**.\n",
    "\n",
    "\n",
    "## Similaridades entre QA e RAG:\n",
    "\n",
    "* Fornecimento de um contexto: Tanto no QA tradicional quanto no RAG, um contexto é fornecido para que o modelo possa responder à pergunta. Esse contexto pode ser um documento, uma passagem ou informações recuperadas de uma base de dados.\n",
    "  \n",
    "* Extração de respostas: Em ambos os casos, o objetivo é fornecer uma resposta baseada em informações relevantes do contexto. No QA, a resposta pode ser um trecho de texto específico extraído diretamente do contexto, enquanto no RAG, o modelo pode gerar respostas com base em informações recuperadas.\n",
    "\n",
    "## Diferenças:\n",
    "* Question Answering: No QA tradicional, o modelo trabalha diretamente com o contexto fornecido e tenta extrair uma resposta específica, normalmente como um \"span\" (trecho) de texto. O modelo não gera novo conteúdo, mas apenas localiza a resposta dentro do texto existente.\n",
    "\n",
    "* RAG (Retrieval-Augmented Generation): RAG combina recuperação de documentos (Retrieval) com geração de texto (Generation). Primeiro, o modelo busca informações relevantes em grandes bases de dados ou fontes externas e, em seguida, gera uma resposta com base nesses documentos recuperados. A principal diferença aqui é que o RAG pode buscar informações além do contexto direto e, depois, gerar uma resposta baseada nesses múltiplos documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e2d6e0-d3fb-4b3a-a9e7-9947e02c0add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\amori\\anaconda3\\envs\\bndes\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instanciando Modelo:\n",
    "reader = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b1d2f7f-9aa3-4176-b024-019547954daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631292</td>\n",
       "      <td>335</td>\n",
       "      <td>358</td>\n",
       "      <td>an exchange of Megatron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  start  end                   answer\n",
       "0  0.631292    335  358  an exchange of Megatron"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizando Pergunta:\n",
    "question = \"What does the customer want?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "pd.DataFrame([outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322278ad-4032-4068-a9c2-8b0a24f49b71",
   "metadata": {},
   "source": [
    "O pipeline também retornou números inteiros de início e fim que correspondem aos índices de caracteres onde o trecho da resposta foi encontrado (assim como acontece com a marcação de **NER**). Existem várias abordagens para _question answerin_ que será investigada em notebook próprio, mas este tipo específico é chamado de **_question answering extrativo_**, pois a resposta é extraída diretamente do texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9284bc-db66-4a0c-9d47-b18d5bef2b3a",
   "metadata": {},
   "source": [
    "# Sumarização (_Summarization_):\n",
    "\n",
    "O objetivo da sumarização de texto é pegar um texto longo como entrada e gerar uma versão curta com todos os **fatos relevantes**. Esta é uma tarefa muito mais complicada do que as anteriores, pois exige que o modelo gere um texto coerente. Seguindo um padrão que já deve ser familiar, podemos instanciar um pipeline de sumarização da seguinte maneira:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ce6c160-e814-46b3-8b4d-3647472e735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\amori\\anaconda3\\envs\\bndes\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instanciando modelo:\n",
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9a2753d-6151-4849-ade4-e63513467e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your min_length=56 must be inferior than your max_length=45.\n",
      "C:\\Users\\amori\\anaconda3\\envs\\bndes\\lib\\site-packages\\transformers\\generation\\utils.py:1282: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (45). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead.\n"
     ]
    }
   ],
   "source": [
    "# Gerando inferência do modelo:\n",
    "outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e5fc67-0a78-4a88-98d0-dc8872559316",
   "metadata": {},
   "source": [
    "Esse resumo não está nada mal! Embora partes do texto original tenham sido copiadas, o modelo conseguiu captar a essência do problema e identificar corretamente que \"Bumblebee\" (que apareceu no final) era o autor da reclamação. Neste exemplo, você também pode ver que passamos alguns argumentos como `max_length` e `clean_up_tokenization_spaces` para o `pipeline`; esses permitem ajustar as saídas em tempo de execução."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40186ffe-f88d-4709-9a01-942bbad386c9",
   "metadata": {},
   "source": [
    "# Tradução:\n",
    "\n",
    "Assim como a sumarização, a tradução é uma tarefa onde a resposta consistem geração de texto. Diga-se de passagem, a arquitetura transformers nasce para essa tarefa, tradução de textos. Os pesquisadores pensaram o modelo para isso, e outras pessoas foram trabalhando a arquitetura para outras tarefas até chegar nesse monte de tarefas que temos hoje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6081c2c4-1dda-4c44-b678-dcae8891122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando Tradutor:\n",
    "translator = pipeline(\"translation_en_to_de\",model=\"Helsinki-NLP/opus-mt-en-de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4c9bd56-3192-4a3e-8959-e3f59f9ffda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sehr geehrter Amazon, letzte Woche habe ich eine Optimus Prime Action Figur aus Ihrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket öffnete, entdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich hoffe, Sie können mein Dilemma verstehen. Um das Problem zu lösen, Ich fordere einen Austausch von Megatron für die Optimus Prime Figur habe ich bestellt. Eingeschlossen sind Kopien meiner Aufzeichnungen über diesen Kauf. Ich erwarte, von Ihnen bald zu hören. Aufrichtig, Bumblebee.\n"
     ]
    }
   ],
   "source": [
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc0b09-6ac0-4e1d-b219-b89ee3446a61",
   "metadata": {},
   "source": [
    "Novamente, o modelo produziu uma tradução muito boa que utilizou corretamente os pronomes formais em alemão, como \"Ihrem\" e \"Sie\". Aqui também mostramos como você pode substituir o modelo padrão no pipeline para escolher o mais adequado para sua aplicação — e você pode encontrar modelos para milhares de pares de idiomas no _Hugging Face Hub_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c366c87-0aff-41c7-91e9-afb895eea56c",
   "metadata": {},
   "source": [
    "# Geração de Texto (_Text Generation_):\n",
    "\n",
    "A grande sereja do bolo, a geração de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfb28110-892d-42c4-8fd2-bd186d8db824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c608ae3d1ffe48798ec8de60d6ae6b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amori\\anaconda3\\envs\\bndes\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\amori\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6809d2a0bcab4a92975290d8ed90f11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601855aabaf544e79fc7eba439caeb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6179b3000be54ce395b8797571e1685a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e10b0bee2164603a8b15eb1d61afb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749881418e4e48a88dca2978ac31cd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573698b9b56d4abd905dee48f9ffeccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instanciando Modelo:\n",
    "generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9468562f-e2a6-4601-ae2e-9c302b8bb596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Amazon, last week I ordered an Optimus Prime action figure\n",
      "from your online store in Germany. Unfortunately, when I opened the package,\n",
      "I discovered to my horror that I had been sent an action figure of Megatron\n",
      "instead! As a lifelong enemy of the Decepticons, I hope you can understand my\n",
      "dilemma. To resolve the issue, I demand an exchange of Megatron for the\n",
      "Optimus Prime figure I ordered. Enclosed are copies of my records concerning\n",
      "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\n",
      "\n",
      "Customer service response:\n",
      "Dear Bumblebee, I am sorry to hear that your order was mixed up. I am an experienced collector of Transformers toys and have been\n",
      "\n",
      "acquainted with many many Autobot figures, and will gladly assist me in finding a new\n",
      "\n",
      "product before Christmas. I am fully on board with the Transformers 5.5 in-house line of figures. I\n",
      "\n",
      "have a special place in my heart for the Optimus Prime Collection, and would happily exchange any\n",
      "\n",
      "customer I will have on sale with it.\n",
      "\n",
      "As an additional precaution, I will send at least six figures from the Transformers 5.5 in-house line\n",
      "\n",
      "with an instruction letter from the Autobot and Decepticon\n",
      "\n",
      "production. These characters represent not only the Autobots, the Autobots and Decepticons, but are the\n",
      "\n",
      "original owners of the Transformers and\n"
     ]
    }
   ],
   "source": [
    "# Gerando Prompt:\n",
    "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "\n",
    "# Inferência do modelo:\n",
    "outputs = generator(prompt, max_length=300)\n",
    "print(outputs[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
