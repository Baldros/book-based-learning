# Deep Learning for Natural Language Processing ğŸ¤–ğŸ’¬

> **"Bridging the gap between Linguistics and Deep Neural Networks"**

Este diretÃ³rio Ã© dedicado ao estudo de arquiteturas neurais profundas aplicadas ao complexo desafio de entender e gerar linguagem humana.

## ğŸ“– O Contexto do Estudo
O processamento de linguagem natural (NLP) passou por uma revoluÃ§Ã£o com o Deep Learning. Este material busca explorar como modelos de redes neurais podem representar o significado das palavras e o contexto das frases de forma computacional.

### ğŸ¯ Objetivos de Estudo
- **RepresentaÃ§Ã£o SemÃ¢ntica**: Entender como transformar palavras em vetores (*embeddings*) que capturam significado.
- **Modelagem Sequencial**: Aprender como os modelos lidam com a ordem das palavras e dependÃªncias de longo prazo.
- **Sistemas de Ponta**: Implementar as bases que sustentam os grandes modelos de linguagem (LLMs) atuais.

### ğŸ’ª Pontos Fortes desta Abordagem
- **Deep Dive TeÃ³rico**: ExploraÃ§Ã£o dos fundamentos das RNNs e LSTMs antes de pular para Transformers.
- **PreparaÃ§Ã£o de Dados**: Foco pesado em prÃ©-processamento (TokenizaÃ§Ã£o, Padding, Limpeza), que Ã© 80% do trabalho em NLP.
- **EvoluÃ§Ã£o**: Acompanha a transiÃ§Ã£o de mÃ©todos estatÃ­sticos tradicionais para redes neurais profundas.

## ğŸ§  TÃ³picos Principais
- **Word Embeddings**: Word2Vec, GloVe e o conceito de espaÃ§o vetorial.
- **Redes Recorrentes**: O uso de memÃ³ria em modelos neurais para processar sequÃªncias.
- **PreparaÃ§Ã£o de Corpus**: Como transformar textos brutos em tensores prontos para o treinamento.

## ğŸ“ ConteÃºdo Organizacional
- **Notebooks de AquisiÃ§Ã£o**: Scripts para coletar e limpar dados textuais.
- **Experimentos**: Testes de arquiteturas para classificaÃ§Ã£o e anÃ¡lise de texto.
