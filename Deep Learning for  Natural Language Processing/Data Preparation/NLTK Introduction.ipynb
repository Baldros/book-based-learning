{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ea1d7c-f814-4353-adfb-8a933135b11c",
   "metadata": {},
   "source": [
    "# Apresentação:\n",
    "\n",
    "O objetivo desse código é explorar um pouco sobre a Biblioteca NLTK utilizando o livro [Deep Learning forNatural Language Processing](https://oku.ozturkibrahim.com/docs_python/Deep_Learning_for_Natural_Language_Processing.pdf).\n",
    "\n",
    "## NLTK:\n",
    "\n",
    "O **NLTK (Natural Language Toolkit)** é uma plataforma líder em Python para trabalhar com dados de linguagem humana, oferecendo interfaces fáceis de usar para mais de 50 corpora e recursos lexicais, como o **WordNet**. Ele fornece uma ampla gama de bibliotecas para tarefas de processamento de texto, incluindo classificação, tokenização, stemming, etiquetagem, análise sintática e raciocínio semântico, além de integrações com bibliotecas NLP industriais. Adequado para linguistas, engenheiros, estudantes, educadores e pesquisadores, sendo uma ferramenta gratuita, open source e amplamente utilizada na área de linguística computacional.\n",
    "\n",
    "## Documentação:\n",
    "\n",
    "* https://www.nltk.org/\n",
    "* https://scikit-learn.org/stable/\n",
    "* https://keras.io/api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3cab542-e20d-4723-bdd4-ec9b551734bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando nltk:\n",
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376b50a-5713-4ede-934f-8de79ce62c9f",
   "metadata": {},
   "source": [
    "# Aquisitando Textos:\n",
    "\n",
    "Antes de qualquer coisa, precisamos aquisitar os textos. O arquivo de texto utilizado aqui já foi previamente montado, então basta lê-lo no código e já o suficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fab39839-feec-48ca-a0ba-17d77ade8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo Arquivos:\n",
    "with open('Metamorphosis','r', encoding='utf-8') as arquivo:\n",
    "    text = arquivo.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec928dc5-1acd-42c3-9015-80a7ac181123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg eBook of Metamorphosis\n",
      "\n",
      "    \n",
      "\n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "\n",
      "you will have to check the laws of the country where you are located\n",
      "\n",
      "before using this eBook\n"
     ]
    }
   ],
   "source": [
    "# Inicio do texto:\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c180b626-1ed4-4321-b9a4-29924833ecb6",
   "metadata": {},
   "source": [
    "# Limpando dados com o NLTK:\n",
    "\n",
    "Computadores não lêem, computadores são máquinas de calcular, de computar. Deste modo, faz-se necessário tratar os textos, vetores de caracteres, de modo que eles possam ser codificados para computação. O primeiro passo desse processo até a codificação de *strings* é a etapa de tratamento do texto bruto, que aqui faremos utilizando o **NLTK**.\n",
    "\n",
    "## Visão Geral:\n",
    "Nessa parte veremos:\n",
    "1. Como limpar os textos usando o NLTK;\n",
    "2. Tokenização manual;\n",
    "3. Tokenização e limpeza com o NLTK;\n",
    "4. Considerações adicionais sobre limpeza de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3210637-eda2-48ad-b96a-b140bba115c4",
   "metadata": {},
   "source": [
    "## Tokenização Manual:\n",
    "\n",
    "Na prática, esse trecho trata de tratamento de *string*. A tokenização é o processo de separação do texto bruto em elementos codificáveis. Ela ainda não se trata de conversão de string em elementos numéritos propriamente dito, porém é a primeira etapa para. Esse processo é uma etapa bem importante onde já se existe muitas coisas sobre. A maioria das LLM's (*Large Language Models*) utilizam Redes Neurais nessa etapa, todavia aqui, a título de estudo, vamos fazer a mão a tokenização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "189ae0c9-1a1e-4eec-827a-33a576776be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'Metamorphosis', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever.', 'You', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.org.', 'If', 'you', 'are', 'not', 'located', 'in', 'the', 'United', 'States,', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'eBook.', '***', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook.', 'Details', 'Below.', '***', '***']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizando por palavra:\n",
    "worlds = text.split();print(worlds[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8565e828-82a0-4597-9fa6-be63ce971e27",
   "metadata": {},
   "source": [
    "Note que essa é uma forma um tanto rudimentar de subdividir, splitar, uma string. Uma outra forma de executar essa tarefa, de forma mais refinada é utilizando **Expressões Regulares**. As **expressões regulares (regex)** são uma sequência de caracteres que define um padrão de pesquisa e são usadas principalmente para correspondência de padrões com cadeias de caracteres. A ideia é utilizar o regex para dividir o documento em palavras por selecionando sequências de caracteres alfanuméricos (a-z, A-Z, 0-9 e \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab65c257-5ad9-43a4-a2e9-d81b6d3d53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando classe de Regex:\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca2f72eb-e2b0-4fb2-a839-b3e79997a741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'The', 'Project', 'Gutenberg', 'eBook', 'of', 'Metamorphosis', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www', 'gutenberg', 'org', 'If', 'you', 'are', 'not', 'located', 'in', 'the', 'United', 'States', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'eBook', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details']\n"
     ]
    }
   ],
   "source": [
    "# splitando os dados apenas em palavras:\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:100]) # Resultado bem mais limpo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d67e1-abac-489d-9647-4407870ffeec",
   "metadata": {},
   "source": [
    "### Retirando Pontuação e espaços:\n",
    "\n",
    "A remoção de espaços em branco e pontuação foi, de fato, uma estratégia comum em etapas de **pré-processamento** em técnicas tradicionais, como **Bag of Words (BoW)** ou **TF-IDF**. No entanto, com o avanço dos modelos de linguagem baseados em redes neurais, como **Transformers** (por exemplo, BERT, GPT, etc.), essa prática tem se tornado menos relevante e até contraproducente em muitos casos. Todavia, seguindo o estudo do livro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14a14a69-8720-4422-96b6-b5eff4bedea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o modulo string\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a26ed-c0aa-4264-a6f6-755154df842c",
   "metadata": {},
   "source": [
    "o módulo `string` é um módulo da biblioteca padrão. Ele não é uma classe, mas sim um módulo que fornece várias constantes e funções relacionadas a strings, que são úteis para operações de manipulação de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4e94b0c-67cb-4f50-b679-ab9536208286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Visualizando elementos:\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acca9846-151d-41d2-b3e0-52f40249f496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'The', 'Project', 'Gutenberg', 'eBook', 'of', 'Metamorphosis', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www', 'gutenberg', 'org', 'If', 'you', 'are', 'not', 'located', 'in', 'the', 'United', 'States', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'eBook', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details']\n"
     ]
    }
   ],
   "source": [
    "# Instanciando estrutura de busca Regex:\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# Removendo pontuação de cada palavra:\n",
    "stripped = [re_punc.sub('', w) for w in words];print(stripped[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4333e0-986d-444f-a2b6-0069221481ee",
   "metadata": {},
   "source": [
    "* `re.compile('[^%s]' % re.escape(string.punctuation))`: Compila uma expressão regular que corresponderá a qualquer caractere que não esteja em `string.punctuation`. O [^...] é um padrão de negação que corresponde a qualquer caractere que não está dentro dos colchetes. Ou `string.printable` como no caso abaixo.\n",
    "\n",
    "* `re_print.sub('', w)`: Substitui todos os caracteres que não são imprimíveis (ou seja, que não estão em string.printable) em cada string w na lista words por uma string vazia (''), efetivamente removendo esses caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccd6d108-1208-4cab-b803-5b027932400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'The', 'Project', 'Gutenberg', 'eBook', 'of', 'Metamorphosis', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www', 'gutenberg', 'org', 'If', 'you', 'are', 'not', 'located', 'in', 'the', 'United', 'States', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'eBook', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details']\n"
     ]
    }
   ],
   "source": [
    "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "result = [re_print.sub('', w) for w in words];print(result[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd93198-2bc8-47b5-a70f-cb82f4b96bb4",
   "metadata": {},
   "source": [
    "* `string.printable`: É uma constante que contém todos os caracteres imprimíveis, incluindo letras, números, pontuação e espaços."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f51560b7-922a-4b2d-83cd-77fd48722741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faade façade\n"
     ]
    }
   ],
   "source": [
    "# Verificando se há diferenças:\n",
    "for i in range(len(result)):\n",
    "    if result[i] != stripped[i]:\n",
    "        print(result[i], stripped[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f191ac-c611-4207-ad2d-e0d4cdb175fd",
   "metadata": {},
   "source": [
    "vemos que o método utilizando o `string.printable` gerou uma \"não palavra\", durante o processo de limpeza. Textos antigos, por vezes, possuem linguagem arcaica, ou seja, palavras estranhas, então é sempre bom verificar. De todo modo, a palavra por sí só é estranha, mas \"façade\" eu encontrei tradução, difente de \"faade\", sendo assim, vou prosseguir com o vetor de strings `stripped`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954b65b7-0078-4396-b600-362657d23f19",
   "metadata": {},
   "source": [
    "### Normalizando Caso:\n",
    "\n",
    "Esse é outro conceito que não é mais tão usando hoje em dia. A *case normalization* é o ato de nivelar as letras numa mesma caixa, geralmente tudo minúsculo. Ou seja, o modelo perde a capacidade de ser *case sensitive*, o que não há mais a necessidade de se fazer hoje em dia. Mas, seguindo a explicação do livro, vamos realizar a normalização com o método `.lower()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c7b4695-2f17-40dc-ac8b-e41f80e5975d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'the', 'project', 'gutenberg', 'ebook', 'of', 'metamorphosis', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www', 'gutenberg', 'org', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook', 'details']\n"
     ]
    }
   ],
   "source": [
    "# Normalizando palavras:\n",
    "strippedLower =[word.lower() for word in stripped]\n",
    "print(strippedLower[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c4354a-4d36-4490-8e31-789fd6acbaf4",
   "metadata": {},
   "source": [
    "## Limpando e Tokenizando com o NLTK:\n",
    "\n",
    "A ideia agora é limpar e tokenizar utilizando o **NLTK**. Não tera exatamente alteração das técnicas, que por sí só são arcaicas dado os modelos transformers, porém, em tese, o **NLTK** é muito útil para realizar o tratamento de *strings*, algo que é de fato penosos, até mesmo utilizando regex.ords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b1442-2a74-432c-ad23-5e073055a02b",
   "metadata": {},
   "source": [
    "### Dividindo em sentenças:\n",
    "\n",
    "Um bom primeiro passo útil é dividir o texto em frases. Algumas tarefas de modelagem preferem que as entradas estejam na forma de parágrafos ou frases, como **Word2Vec**. Poderia-se primeiro dividir texto em frases, dividindo cada frase em palavras e salvando-as em arquivos, cada frase por linha. **NLTK** fornece a funçnão `tokenizer()` construída para dividir o texto em frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4e3a819-c676-49bb-8cd2-f40d688af3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando tokenizador por sentença:\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "548f6f45-c885-4639-b126-2e3fffb73a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg eBook of Metamorphosis\n",
      "\n",
      "    \n",
      "\n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "\n",
      "whatsoever.\n"
     ]
    }
   ],
   "source": [
    "# split em sentenças\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9936b39a-caad-4fb5-af4c-5e26841e7a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-------------------------------------------------------------------------#\n",
      "﻿The Project Gutenberg eBook of Metamorphosis\n",
      "\n",
      "    \n",
      "\n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "\n",
      "whatsoever.\n",
      "#-------------------------------------------------------------------------#\n",
      "#-------------------------------------------------------------------------#\n",
      "You may copy it, give it away or re-use it under the terms\n",
      "\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "\n",
      "at www.gutenberg.org.\n",
      "#-------------------------------------------------------------------------#\n",
      "#-------------------------------------------------------------------------#\n",
      "If you are not located in the United States,\n",
      "\n",
      "you will have to check the laws of the country where you are located\n",
      "\n",
      "before using this eBook.\n",
      "#-------------------------------------------------------------------------#\n",
      "#-------------------------------------------------------------------------#\n",
      "*** This is a COPYRIGHTED Project Gutenberg eBook.\n",
      "#-------------------------------------------------------------------------#\n",
      "#-------------------------------------------------------------------------#\n",
      "Details Below.\n",
      "#-------------------------------------------------------------------------#\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"#-------------------------------------------------------------------------#\")\n",
    "    print(sentences[i])\n",
    "    print(\"#-------------------------------------------------------------------------#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a787cd-aba9-4bac-90d9-e0799c65c931",
   "metadata": {},
   "source": [
    "### Tokenizando por palavras:\n",
    "\n",
    "Assim como tokenizamos por frases, poderiamos tokenizar por palavras, para isso usamos as função `word_tokenize()` que divide *strings* em tokens (nominalmente palavras). Ele divide os tokens com base em espaços em branco e pontuação. Por exemplo, vírgulas e os períodos são considerados tokens separados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3609116e-b62b-44ac-a1c4-cd456ccb92c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando tokenizador por palavras:\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a2d8d26-1a92-4be2-b093-24b6186971bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'Metamorphosis', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.org', '.', 'If', 'you', 'are', 'not', 'located', 'in', 'the', 'United', 'States', ',', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'eBook', '.', '*', '*', '*', 'This', 'is', 'a', 'COPYRIGHTED']\n"
     ]
    }
   ],
   "source": [
    "# splitando por palavras:\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75328ec-9e99-450b-8b4b-e17e2b0ccfb7",
   "metadata": {},
   "source": [
    "###  Filtrando Pontuação:\n",
    "\n",
    "Podemos filtrar todos os tokens nos quais não estamos interessados, como toda pontuação independente. Esse pode ser feito iterando todos os tokens e mantendo apenas os tokens que estão todos em ordem alfabética. Python tem a função `isalpha()` que pode ser usada para tal, (para mais informações, acesso a [página do método](https://docs.python.org/3/library/stdtypes.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98a39d74-3fa4-45f3-a6d1-c637ec0174b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Project', 'Gutenberg', 'eBook', 'of', 'Metamorphosis', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'If', 'you', 'are', 'not', 'located', 'in', 'the', 'United', 'States', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'eBook', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details', 'Below', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in']\n"
     ]
    }
   ],
   "source": [
    "# splitando em palavras:\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Removendo tokens que não são alfabéticos:\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359d2f00-51b0-41b3-9af2-6f5e851b0471",
   "metadata": {},
   "source": [
    "### Filtrando palavras irrelevantes\n",
    "\n",
    "Palavras irrelevantes são aquelas palavras que não contribuem para o significado mais profundo da frase. Eles\n",
    "são as palavras mais comuns, como: the, a e is. Para algumas aplicações como documentação\n",
    "classificação, pode fazer sentido remover palavras irrelevantes. NLTK fornece uma lista de comumente\n",
    "concordaram em palavras de parada para uma variedade de idiomas, como o inglês.\n",
    "\n",
    "É importante dizer que essa é mais uma importante página virada com a elaboração dos modelos de LLM, porque, devido ao mecânismo de atenção, todas essas questões são resolvidas no próprio modelo, sem que haja a necessidade de ser considerada no processo de pré-processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48659377-f33b-4de4-9b26-a344b3a6274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando:\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "39ad41cd-fd01-4763-9f4c-013be72db8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Separando por palavras:\n",
    "stop_words = stopwords.words('english') # Funciona pra portugues também!\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a94cdbd-4837-4ff1-bb23-393da471a448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Project', 'Gutenberg', 'eBook', 'Metamorphosis', 'This', 'ebook', 'use', 'anyone', 'anywhere', 'United', 'States', 'parts', 'world', 'cost', 'almost', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'give', 'away', 'terms', 'Project', 'Gutenberg', 'License', 'included', 'ebook', 'online', 'If', 'located', 'United', 'States', 'check', 'laws', 'country', 'located', 'using', 'eBook', 'This', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details', 'Below', 'Please', 'follow', 'copyright', 'guidelines', 'file', 'Title', 'Metamorphosis', 'Author', 'Franz', 'Kafka', 'Translator', 'David', 'Wyllie', 'Release', 'date', 'August', 'eBook', 'Most', 'recently', 'updated', 'April', 'Language', 'English', 'START', 'OF', 'THE', 'PROJECT', 'GUTENBERG', 'EBOOK', 'METAMORPHOSIS', 'Metamorphosis', 'Franz', 'Kafka', 'Translated', 'David', 'Wyllie', 'I', 'One', 'morning', 'Gregor', 'Samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'He', 'lay', 'back', 'lifted', 'head']\n"
     ]
    }
   ],
   "source": [
    "# Filtrando palavras irrelevantes:\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694c7fd-6b5a-47b9-95ff-cac52fb6189b",
   "metadata": {},
   "source": [
    "### Reduzindo ao elemento morfológico fundamental:\n",
    "\n",
    "***Stemming*** refere-se ao processo de reduzir cada palavra à sua **raiz (_Word Stem_)** ou base. Por exemplo, fishing (pescando), fished (pescou) e fisher (pescador) são todos reduzidos ao radical fish (peixe). Algumas aplicações, como a classificação de documentos, podem se beneficiar do *stemming* para reduzir o vocabulário e focar no sentido ou sentimento de um documento, em vez de um significado mais profundo. Existem muitos algoritmos de *stemming*, embora um método popular e de longa data seja o ***Porter Stemming Algorithm***. Esse método está disponível no NLTK através da classe `PorterStemmer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e34badbe-2387-4ab8-8ad1-9aa27e1bbcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando Classe que realiza a redução da palavra\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fc896a65-79cc-4489-9864-75b291be119c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'metamorphosi', 'thi', 'ebook', 'use', 'anyon', 'anywher', 'unit', 'state', 'part', 'world', 'cost', 'almost', 'restrict', 'whatsoev', 'you', 'may', 'copi', 'give', 'away', 'term', 'project', 'gutenberg', 'licens', 'includ', 'ebook', 'onlin', 'if', 'locat', 'unit', 'state', 'check', 'law', 'countri', 'locat', 'use', 'ebook', 'thi', 'copyright', 'project', 'gutenberg', 'ebook', 'detail', 'below', 'pleas', 'follow', 'copyright', 'guidelin', 'file', 'titl', 'metamorphosi', 'author', 'franz', 'kafka', 'translat', 'david', 'wylli', 'releas', 'date', 'august', 'ebook', 'most', 'recent', 'updat', 'april', 'languag', 'english', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', 'metamorphosi', 'metamorphosi', 'franz', 'kafka', 'translat', 'david', 'wylli', 'i', 'one', 'morn', 'gregor', 'samsa', 'woke', 'troubl', 'dream', 'found', 'transform', 'bed', 'horribl', 'vermin', 'he', 'lay', 'back', 'lift', 'head']\n"
     ]
    }
   ],
   "source": [
    "# Instanciando Classe:\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Resultado Final:\n",
    "stemmed = [porter.stem(word) for word in words]\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e1c0fe-d8d6-4ba1-8310-4f3308be2c1d",
   "metadata": {},
   "source": [
    "# Considerações Finais:\n",
    "\n",
    "Podemos então resumir o processo como:\n",
    "\n",
    "1. Carregar o texto bruto;\n",
    "2. Dividir em tokens;\n",
    "3. Converter para minúsculas;\n",
    "4. Remover a pontuação de cada token;\n",
    "5. Filtrar os tokens restantes que não são alfabéticos;\n",
    "6. Filtrar tokens que são stop words;\n",
    "7. Resumir ao elemento morfológico fundamental. (Opcional)\n",
    "\n",
    "Note que o texto-fonte utilizado no estudo estava razoavelmente limpo desde o início, não havendo a necessidade então de passar por muitas preocupações com a limpeza de texto que pode-se precisar lidar em um projeto real. É valido listar alguns desses problemas:\n",
    "\n",
    "* Lidar com documentos grandes e grandes coleções de documentos de texto que não cabem na memória.\n",
    "* Extrair texto de marcações como HTML, PDF ou outros formatos de documentos estruturados.\n",
    "* Transliteração de caracteres de outros idiomas para o inglês.\n",
    "* Decodificação de caracteres Unicode em uma forma normalizada, como UTF-8.\n",
    "* Tratamento de palavras, frases e siglas específicas de domínio.\n",
    "* Tratamento ou remoção de números, como datas e quantias.\n",
    "* Localização e correção de erros comuns de digitação e ortografia. E muito mais...\n",
    "\n",
    "Uma dica profissional que o texto-base (livro) deixa, é revisar continuamente os tokens gerados após cada transformação. Idealmente, pode-se salvar um novo arquivo após cada transformação para que se possa passar um tempo com todos os dados na nova forma. As coisas sempre saltam aos olhos quando você toma o tempo para revisar seus dados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
